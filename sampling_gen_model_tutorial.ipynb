{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling with the help of a generative model\n",
    "\n",
    "Marylou Gabrié, École Polytechnique, CMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will go a simple of example of how we can leverage generative models to assist the computation of expectations via Monte Carlo. Three modules are coming with this notebook to ease implementation:\n",
    "- `models.py`\n",
    "- `utils_plot.py`\n",
    "- `utils_mcmc.py`\n",
    "\n",
    "In you cloned the repository locally, you should have no issue loading these modules with the `import` commands as done below. On Google Colab however you will need to run the following cell before running the one following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/MyDrive/tutorial-sampling-enhanced-w-generative-models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models import NormalizingFlow, MoG\n",
    "from utils_plot import plot_density, grab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - A mixture of two Gaussians in 2d as a simple toy-target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generically speaking, a mixture of Gaussian is a density defined as a weighted sum of Gaussian densities. For Gaussian mixtture with $k$-components in $d$-dimension has density at $x\\in \\mathbb{R}^d$:\n",
    "\n",
    "$\n",
    "\\displaystyle \\rho(x) = \\sum_{i=1}^k \\frac{\\alpha_i}{\\sqrt{(2 \\pi)^{d} |\\Sigma|}} \\; \\exp\\left[-\\frac{1}{2} (x-\\mu_i)^T \\Sigma_i (x - \\mu_i)\\right],\n",
    "$\n",
    "\n",
    "where $\\mu_i \\in \\R^d$ and $\\Sigma_i \\in \\R^{d\\times d}$ are respectivally the mean and covariance of component $i$ and the weights $\\alpha_i \\in [0,1]$ sum to 1. Here we can simply start with a 2d target distribution with 2 Gaussian components with covariances identity and imbalance weights: \n",
    "\n",
    "$\n",
    "\\displaystyle \\rho_*(x) = \\frac{\\alpha_1}{2 \\sqrt{2 \\pi}} \\; \\exp\\left[-\\frac{1}{2} \\Vert x - \\mu_1 \\Vert^2 \\right] + \\frac{\\alpha_2}{2 \\sqrt{2 \\pi}} \\; \\exp\\left[-\\frac{1}{2} \\Vert x - \\mu_2 \\Vert^2\\right],\n",
    "$\n",
    "with say $\\alpha_1 = 0.25$ and $\\alpha_2 = 0.75$.\n",
    "\n",
    "A class `MoG` is defined in the module `models.py` to handle mixture of Gaussians. Let's instantiate the target and plot its density using a function from the module `utils_plot.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 1 # position of Gaussian components\n",
    "scale = 0.1 # scale of Gaussian components\n",
    "covars = [torch.eye(2) * scale, torch.eye(2) * scale]\n",
    "means = [torch.tensor([-pos, - pos]), torch.tensor([pos, pos])]\n",
    "target = MoG(means, covars, weights=[0.25, 0.75])\n",
    "\n",
    "# plot the target \n",
    "lims = {'x_min': 2 * - pos, 'x_max': 2 * pos, 'y_min': - 2 * pos, 'y_max': 2 * pos}\n",
    "plot_density(target.log_prob, lims=lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - A Normalizing Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing flows form a class of generative models that can be used to approximate a target distribution. Again, a ready-made class of flows is provided in the module `models.py`. The class provides a method `log_prob` to compute the log-probability of a batch of samples and a method `sample` to draw samples from the flow. \n",
    "\n",
    "The flow defines a parametrized density $\\rho_\\theta$ trough the transformation of an isotropic Gaussian distribution through a parametrized map which we denote by $T_\\theta: \\mathbb{R}^d \\to \\mathbb{R}^d$. This map is invertible: $T_\\theta(T^{-1}_\\theta(x)) = T^{-1}_\\theta(T_\\theta(x)) = x$. \n",
    "\n",
    "Let's instantiate a simple flow with 2 blocks of 2 Affine Coupling layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "n_blocks = 2\n",
    "\n",
    "model = NormalizingFlow(dim, n_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check that the flow is invertible using the `model.forward` and `model.backward` methods.\n",
    "\n",
    "- How can you check that the flow is initialized with a $T_\\theta$ that is the identity map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Training the flow to approximate the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the superposition of the target and the flow density as initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density(model.log_prob, target.log_prob, lims=lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our goal is to train the flow to approximate the target. We first need a loss function. The Kullback-Leibler divergence between the flow and the target is a good candidate:\n",
    "\n",
    "$\n",
    "{\\rm KL}(\\rho_\\theta || \\rho_*) =  \\int  \\rho_\\theta(x)  \\log \\left(\\frac{\\rho_\\theta(x)}{\\rho_*(x)} \\right) dx.\n",
    "$\n",
    "\n",
    "Recall that the $\\rm KL$ is always positive and zero if only if $\\rho_\\theta = \\rho_*$. We cannot compute it exactly but we can approximate it using Monte Carlo and samples from the flow: our loss function is then\n",
    "\n",
    "$\n",
    "\\mathcal{L}(\\theta) = \\frac{1}{B} \\sum_{k=1}^B \\left[ \\log \\left(\\frac{\\rho_\\theta(x^k)}{\\rho_*(x^k)} \\right) \\right]\n",
    "$\n",
    "\n",
    "where $x^k$ are samples from the flow and $B$ is the batch size. \n",
    "- Implement the loss function in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kl_divergence(model, target, batch_size=100):\n",
    "    #\n",
    "    return # a torch.tensor of one element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we wrote a function with a simple training loop. Check that everything makes sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs=100, lr=1e-3, batch_size=100):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    axs = [plt.subplot(2, 5, i) for i in range(1, 11)]\n",
    "    models = []\n",
    "    losses = []\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = kl_divergence(model, target, batch_size=batch_size)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "       \n",
    "        if epoch % (n_epochs/10) == 0:\n",
    "            print(f'Epoch {epoch}: {loss.item()}')\n",
    "            ax_index = int(epoch // (n_epochs/10))\n",
    "            plot_density(model.log_prob, target.log_prob, lims=lims, \n",
    "                         ax=axs[ax_index], title=f'Epoch {epoch}')\n",
    "    return losses\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the training now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)  #fix seed for reproducibility/comment to compare different runs\n",
    "dim = 2\n",
    "n_blocks = 2\n",
    "model = NormalizingFlow(dim, n_blocks)\n",
    "\n",
    "losses = train(model, n_epochs=50, lr=5e-3) \n",
    "# increase to 500 iterations to get better results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the losses during training to make sure that everything is going well. What would be the ideal value of the loss at the end of training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the superposition of the samples from the flow and the target density. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 -  Importance sampling with a trained flow as proposal\n",
    "\n",
    "Although the flow is not perfect at approximating the target, it can still be of great help to compute expectations. Here we will demonstrate how we can use the flow to compute expectations via importance sampling.\n",
    "\n",
    "The importance sampling algorithm follows the following logic:\n",
    "- Draw samples from a proposal distribution easy to sample and for which you can write the density( here we can use the flow): $\\quad x_i \\sim \\rho_\\theta(x), \\quad$ for $i = 1, \\ldots, N$.\n",
    "\n",
    "- Compute the importance weights: $\\quad \\omega_i = \\frac{\\rho_*(x_i)}{\\rho_\\theta(x_i)}, \\quad$ for $i = 1, \\ldots, N$.\n",
    "\n",
    "- Approximate thermodynamic averages of any observable $\\mathcal{O}(\\cdot)$ under $\\rho_*(\\cdot)$ as: $\\quad \\displaystyle \\langle \\mathcal{O}(x) \\rangle_* \\approx \\sum_{i = 1}^N \\frac{w_i \\mathcal{O}(x_i)}{\\sum_{j=1}^N w_j}  $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we visualize the importance weights for the samples drawn from the flow. The size of the black dots is proportional to the importance weight. Check that it fits your intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = model.sample(10000)\n",
    "log_ws = target.log_prob(xs) - model.log_prob(xs)  \n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.subplot(111)\n",
    "plot_density(target.log_prob, lims=lims, ax=ax, title='Target')\n",
    "plt.scatter(grab(xs[:, 0]), grab(xs[:, 1]), alpha=0.1) \n",
    "plt.scatter(grab(xs[:, 0]), grab(xs[:, 1]), s=grab(torch.exp(log_ws)), c='k')\n",
    "plt.gca().set_xlim(lims['x_min'], lims['x_max'])\n",
    "plt.gca().set_ylim(lims['y_min'], lims['y_max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize the weights, we should use the numerically stable [`logsumexp`](https://pytorch.org/docs/stable/generated/torch.logsumexp.html),  (click to check the documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_normalized_ws = log_ws - torch.logsumexp(log_ws, dim=0)\n",
    "normalized_ws = torch.exp(log_normalized_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By summing the corresponding weights, we can compute the weight of each Gaussian component. For example, we can compute the weight of the first Gaussian component by filtering the weighted samples in the positive quadrant of the plane. Check that the result makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_mode_pos = normalized_ws[(xs[:, 0] > 0) * (xs[:, 1] > 0)].sum()\n",
    "print('We predict a weight for the mode in the positive cadrant of:', weight_mode_pos.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For this simple 2d problem, check that we could have obtained a similar result by using a simpler proposal distribution than the trained normalizing flow (e.g. uniform or isotropic Gaussian).\n",
    "- Do you have an intuition of when these simple strategies break?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a different simple proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Markov Chain Monte Carlo with a trained flow as proposal\n",
    "\n",
    "We can also leverage the flow as a proposal distribution in a Metropolis-Hastings MCMC. The algorithm is as follows:\n",
    "- Intialize $x^{(0)}$ at random.\n",
    "- For $t = 1, \\ldots, T$:\n",
    "    - Draw a candidate sample $x^{(t)}$ from the flow: $\\quad x^{(t)} \\sim \\rho_\\theta(x)$.\n",
    "    - Accept the candidate with probability:  $\\quad \\min\\left[1, \\frac{\\rho_*(x^{(t)}) \\rho_\\theta(x^{(t-1)})}{\\rho_*(x^{(t-1)})\\rho_\\theta(x^{(t)})} \\right]$.\n",
    "    - If rejected, set:  $\\quad x^{(t)} = x^{(t-1)}$.\n",
    "\n",
    "The Markov Chain $(x^{(t)})_{t=1}^T$ (once converged) provides a sample from the target distribution $\\rho_*(\\cdot)$.\n",
    "\n",
    "- Implement the MCMC algorithm described above. As an example, we provide a function implementing the Metropolis Adjusted Langevin Algorithm (MALA) in the module `utils_mcmc.py`. You can use it as a reference. We also provide a visualization of the samples from the MALA MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_mcmc import run_mala, get_grad_U\n",
    "\n",
    "torch.manual_seed(10)  #fix seed for reproducibility/comment to compare different runs\n",
    "\n",
    "U = lambda x: - target.log_prob(x)\n",
    "grad_U = get_grad_U(U)\n",
    "\n",
    "n_samples = 5\n",
    "x_init = torch.randn(n_samples, 2)\n",
    "\n",
    "xs, accs = run_mala(U, grad_U, x_init, n_steps=100, dt=0.05)\n",
    "\n",
    "plot_density(target.log_prob, lims=lims)\n",
    "plt.plot(grab(xs[:, :, 0]), grab(xs[:, :, 1]), '.-', alpha=0.5)\n",
    "\n",
    "print('Mean acceptance rate:', (accs * 1.).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "- [1] Albergo et al. \"Flow-based generative models for Markov chain Monte Carlo in lattice field theory\" (2019) (https://arxiv.org/abs/1904.12072)\n",
    "- [2] Gabrié et al. \"Adaptive Monte Carlo augmented with normalizing flows\" (2021) (https://arxiv.org/abs/2105.12603)\n",
    "and references therein!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5acfc255139a090c95682319884ba928066bcd183f9ed320ebbfb104e79bd341"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
